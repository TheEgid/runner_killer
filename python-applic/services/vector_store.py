import os
import time
from supabase import create_client, Client # type: ignore
from typing import List, Dict, Any
from services.local_embedder import LocalCohereClient
# import import cohere # type: ignore

from models import SearchResult


class VectorStoreService:
    def __init__(self, logger):
        self.logger = logger
        self.supabase_url = os.getenv("SUPABASE_URL")
        self.supabase_key = os.getenv("SUPABASE_KEY")
        # self.cohere_api_key = os.getenv("COHERE_API_KEY")

        if not all([self.supabase_url, self.supabase_key]):
            missing = [var for var, val in [
                ("SUPABASE_URL", self.supabase_url),
                ("SUPABASE_KEY", self.supabase_key),
                # ("COHERE_API_KEY", self.cohere_api_key)
            ] if not val]
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {', '.join(missing)}")

        try:
            self.supabase: Client = create_client(self.supabase_url, self.supabase_key)
            # self.cohere_client = cohere.Client(self.cohere_api_key)
            self.cohere_client = LocalCohereClient()
            self.logger.info("‚úÖ VectorStoreService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ VectorStoreService: {e}")
            raise

    def add_chunks(self, chunks: List[Dict[str, Any]]) -> bool:

        if not chunks:
            self.logger.warning("‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ –¥–æ–±–∞–≤–∏—Ç—å –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤")
            return False

        try:
            self.logger.info(f"üìù –î–æ–±–∞–≤–ª—è–µ–º {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î...")

            texts = [chunk["text"] for chunk in chunks if chunk.get("text")]
            if not texts:
                self.logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
                return False

            # response = self.cohere_client.embed(
            #     texts=texts,
            #     model="embed-multilingual-light-v3.0",
            #     input_type="search_document"
            # )
            # embeddings = response.embeddings embed_documents
            response = self.cohere_client.embed_documents(texts=texts)
            embeddings_list = response.embeddings.tolist()

            # ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            if len(embeddings_list) == 0:
                self.logger.error("‚ùå –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—É—Å—Ç—ã–µ")
                return False

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø–µ—Ä–≤–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            if len(embeddings_list[0]) != 384:
                self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞: {len(embeddings_list)}, –æ–∂–∏–¥–∞–µ—Ç—Å—è 384")
                return False

            # if embeddings and len(embeddings[0]) != 384:
            #     self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(embeddings[0])}, –æ–∂–∏–¥–∞–µ—Ç—Å—è 384")
            #     return False

            rows = []
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings_list)):
                # –ë–µ—Ä–µ–º –º–µ—Ç–∞–¥–∞—Ç—É –∏–∑ —á–∞–Ω–∫–∞, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
                metadata = dict(chunk.get("metadata", {}))

                if "loc" not in metadata:
                    chunk_size = len(chunk["text"].splitlines()) or 1
                    start_line = i * chunk_size + 1
                    end_line = (i + 1) * chunk_size
                    metadata["loc"] = {"lines": {"from": start_line, "to": end_line}}

                # –î–æ–ø–æ–ª–Ω—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ –º–µ—Ç–∞–¥–∞—Ç–µ
                metadata.setdefault("source", chunk.get("source", "blob"))
                metadata.setdefault("blobType", chunk.get("blob_type", "text/plain"))
                metadata.setdefault("chunk_index", chunk.get("chunk_index", i))
                metadata.setdefault("total_chunks", chunk.get("total_chunks", len(chunks)))

                # URL —É–∂–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –º–µ—Ç–∞–¥–∞—Ç–µ –∏–∑ _smart_chunk_content
                # –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
                if "url" not in metadata and "url" in chunk:
                    metadata["url"] = chunk["url"]

                rows.append({
                    "content": chunk["text"],
                    "metadata": metadata,
                    "embedding": embedding
                })

            result = self.supabase.table("novaya").upsert(rows).execute()

            if result.data:
                self.logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(result.data)} —á–∞–Ω–∫–æ–≤")
                return True
            else:
                self.logger.error("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –ë–î")
                return False

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —á–∞–Ω–∫–æ–≤: {e}")
            return False


    def search(self, query: str, top_k: int = 5, similarity_threshold: float = 0.5) -> List[SearchResult]:
        """
        ‚úÖ –ü–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SQL —Ñ—É–Ω–∫—Ü–∏–∏
        """
        if not query or not query.strip():
            self.logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞")
            return []

        try:
            self.logger.info(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query[:50]}...'")

            # # ‚úÖ –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 384)
            query_response = self.cohere_client.embed_query(query)
            query_embedding_list = query_response.tolist()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            if len(query_embedding_list) != 384:
                self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞: {len(query_embedding_list)}, –æ–∂–∏–¥–∞–µ—Ç—Å—è 384")

            # query_response = self.cohere_client.embed(
            #     texts=[query],
            #     model="embed-multilingual-light-v3.0",
            #     input_type='search_query'
            # )
            # query_embedding = query_response.embeddings[0]
            # if len(query_embedding) != 384:
            #     self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å query —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(query_embedding)}")
            #     return []

            # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º RPC —Ñ—É–Ω–∫—Ü–∏—é
            result = self.supabase.rpc(
                'match_documents_novaya_v2',
                {
                    'query_embedding': query_embedding_list,
                    'match_threshold': similarity_threshold,
                    'match_count': top_k
                }
            ).execute()

            if not result.data:
                self.logger.info("‚ÑπÔ∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                return []

            # ‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            search_results = []
            for row in result.data:
                search_results.append(SearchResult(
                    content=row['content'],
                    score=row.get('similarity', 0.0),
                    metadata=row.get('metadata', {}),
                    id=str(row.get('id', ''))
                ))

            self.logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return search_results

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            # ‚úÖ –ü—Ä–æ—Å—Ç–æ–π fallback
            return self._text_search_fallback(query, top_k)

    def _text_search_fallback(self, query: str, top_k: int) -> List[SearchResult]:
        """
        ‚úÖ –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∫–∞–∫ fallback
        """
        try:
            self.logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∫–∞–∫ fallback...")

            # –ü–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            result = (
                self.supabase.table("novaya")
                .select("id, content, metadata")
                .ilike("content", f"%{query}%")
                .limit(top_k)
                .execute()
            )

            search_results = []
            if result.data:
                for row in result.data:
                    search_results.append(SearchResult(
                        content=row['content'],
                        score=0.5,  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
                        metadata=row.get('metadata', {}),
                        id=str(row.get('id', ''))
                    ))

            self.logger.info(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫: –Ω–∞–π–¥–µ–Ω–æ {len(search_results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return search_results

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def url_exists(self, url: str, retries: int = 3, delay: float = 1.0) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è URL —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        if not url:
            return False

        for attempt in range(1, retries + 1):
            try:
                result = (
                    self.supabase.table("novaya")
                    .select("id")
                    .or_(f"metadata->>URL.eq.{url},metadata->>url.eq.{url}")
                    .limit(1)
                    .execute()
                )
                return bool(result.data)
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ URL {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt}): {e}")
                if attempt < retries:
                    time.sleep(delay)
                else:
                    return False

    def question_exists(self, data: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –ª–∏ data –≤ –ø–æ–ª–µ content"""
        if not data:
            return False

        try:
            result = (
                self.supabase.table("novaya")
                .select("id")
                .like("content", f"%{data}%")  # –∏—â–µ–º –ø–æ–¥—Å—Ç—Ä–æ–∫—É
                .limit(1)
                .execute()
            )
            exists = bool(result.data and len(result.data) > 0)
            return exists

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –≤–æ–ø—Ä–æ—Å–∞ '{data}': {e}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            total_result = self.supabase.table("novaya").select("id", count="exact").execute()
            total_count = total_result.count if total_result.count is not None else 0

            return {
                "total_documents": total_count,
                "database_table": "novaya",
                "embedding_model": "embed-multilingual-light-v3.0",
                "embedding_dimension": 384
            }

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {"error": str(e)}

    def add_documents(self, documents: List[str], metadata_list: List[Dict[str, Any]] = None, chunk_size: int = 10000) -> bool:
        """–ú–µ—Ç–æ–¥ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ —á–∞–Ω–∫–∏"""
        if metadata_list is None:
            metadata_list = [{"source": "direct_add"} for _ in documents]

        chunks = []
        for doc_id, (doc, metadata) in enumerate(zip(documents, metadata_list)):
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ chunk_size —Å–∏–º–≤–æ–ª–æ–≤
            for i in range(0, len(doc), chunk_size):
                chunk_text = doc[i:i+chunk_size]

                # –≤—ã—á–∏—Å–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –¥–ª—è loc
                line_count = len(chunk_text.splitlines()) or 1
                start_line = i // chunk_size * line_count + 1
                end_line = start_line + line_count - 1

                chunks.append({
                    "text": chunk_text,
                    "metadata": metadata,
                    "chunk_index": i // chunk_size,
                    "total_chunks": (len(doc) + chunk_size - 1) // chunk_size,
                    "start_line": start_line,
                    "end_line": end_line
                })

        return self.add_chunks(chunks)
