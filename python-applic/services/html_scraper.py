import asyncio
import socket
import time
from typing import Dict, Any, Optional, List, Tuple
from urllib.parse import urljoin, urlparse
from prefect.utilities.asyncutils import run_coro_as_sync # type: ignore
from bs4 import BeautifulSoup # type: ignore
import requests # type: ignore
import json
from crawl4ai import  AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode # type: ignore

HAS_CLOUDSCRAPER = False
HAS_DNS_RESOLVER = False

class StructuredHTMLScraper:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü.
    –í–∫–ª—é—á–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ fallback —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –¥–æ–º–µ–Ω–æ–≤ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ.
    """

    def __init__(self, logger, headless: bool = True, use_custom_dns: bool = True):
        dns_args = []
        if use_custom_dns:
            dns_args = [
                '--dns-over-https-server=https://1.1.1.1/dns-query',  # Cloudflare DNS
            ]

        self.logger = logger
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±—Ä–∞—É–∑–µ—Ä–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.browser_config = BrowserConfig(
            browser_type="chromium",
            headless=headless,
            verbose=False,
            extra_args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-web-security',
                '--disable-features=IsolateOrigins,site-per-process',
                '--ignore-certificate-errors',
                '--ignore-ssl-errors',
                '--disable-gpu',
                '--disable-extensions',
                '--disable-images',  # –û—Ç–∫–ª—é—á–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                *dns_args
            ]
        )

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_requests': 0,
            'successful': 0,
            'failed': 0,
            'fallback_used': 0,
        }


    def _check_domain_availability(self, url: str) -> Tuple[bool, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–æ–º–µ–Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏.

        Returns:
            tuple[bool, str]: (–¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ –¥–æ–º–µ–Ω, —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ)
        """
        try:
            parsed = urlparse(url)
            domain = parsed.netloc or parsed.path

            # –£–¥–∞–ª—è–µ–º www. –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            domain = domain.replace('www.', '')

            # –ú–µ—Ç–æ–¥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ socket
            try:
                socket.gethostbyname(domain)
                self.logger.info(f"‚úÖ –î–æ–º–µ–Ω {domain} —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑—Ä–µ—à–µ–Ω —á–µ—Ä–µ–∑ socket")
                return True, ""
            except socket.gaierror as e:
                self.logger.warning(f"‚ö†Ô∏è Socket –Ω–µ –º–æ–∂–µ—Ç —Ä–∞–∑—Ä–µ—à–∏—Ç—å {domain}: {e}")

            # # –ú–µ—Ç–æ–¥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ DNS resolver (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
            # if HAS_DNS_RESOLVER:
            #     try:
            #         resolver = dns.resolver.Resolver()
            #         resolver.timeout = 5
            #         resolver.lifetime = 5
            #         resolver.nameservers = ['8.8.8.8', '1.1.1.1', '77.88.8.8', '9.9.9.9']
            #         answers = resolver.resolve(domain, 'A')
            #         if answers:
            #             self.logger.info(f"‚úÖ DNS –∑–∞–ø–∏—Å–∏ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è {domain}")
            #             return True, ""
            #     except dns.resolver.NXDOMAIN:
            #         self.logger.warning(f"‚ö†Ô∏è –î–æ–º–µ–Ω {domain} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (NXDOMAIN)")
            #         return False, f"–î–æ–º–µ–Ω {domain} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"
            #     except dns.resolver.NoNameservers:
            #         self.logger.warning(f"‚ö†Ô∏è –í—Å–µ DNS-—Å–µ—Ä–≤–µ—Ä—ã –≤–µ—Ä–Ω—É–ª–∏ –æ—à–∏–±–∫—É –¥–ª—è {domain}")
            #     except dns.resolver.Timeout:
            #         self.logger.warning(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç DNS-–∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {domain}")
            #     except Exception as e:
            #         self.logger.warning(f"‚ö†Ô∏è DNS resolver –æ—à–∏–±–∫–∞ –¥–ª—è {domain}: {e}")

            # –ú–µ—Ç–æ–¥ 3: HTTP –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ requests
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.head(url, timeout=5, allow_redirects=True, headers=headers, verify=False)
                if response.status_code < 500:
                    self.logger.info(f"‚úÖ HTTP –ø—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–∞ –¥–ª—è {url} (–∫–æ–¥: {response.status_code})")
                    return True, ""
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è HTTP –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {url}: {e}")

            return False, f"–î–æ–º–µ–Ω {domain} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –≤—Å–µ–º –º–µ—Ç–æ–¥–∞–º –ø—Ä–æ–≤–µ—Ä–∫–∏"

        except Exception as e:
            return False, f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–º–µ–Ω–∞: {e}"

    def validate_and_fix_url(self, url: str) -> Optional[str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –ø—ã—Ç–∞–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å URL, –ø—Ä–æ–±—É—è —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã.
        """
        # –ë–∞–∑–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã URL
        variants = [
            url,
            url.replace('https://', 'http://'),
            f"https://www.{url.replace('https://', '').replace('http://', '').replace('www.', '')}",
            f"http://www.{url.replace('https://', '').replace('http://', '').replace('www.', '')}",
        ]

        for variant in variants:
            is_available, _ = self._check_domain_availability(variant)
            if is_available:
                self.logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ä–∞–±–æ—á–∏–π –≤–∞—Ä–∏–∞–Ω—Ç URL: {variant}")
                return variant

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º web.archive.org –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑–µ—Ä–≤
        self.logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Ä–∞–±–æ—á–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –¥–ª—è {url}, –ø—Ä–æ–±—É–µ–º Internet Archive")
        archive_url = f"https://web.archive.org/web/2/{url}"
        return archive_url

    async def _fallback_scraping(self, url: str) -> Optional[Dict[str, Any]]:
        """
        Fallback –º–µ—Ç–æ–¥—ã —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ cloudscraper –∏ requests.
        """
        self.logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –º–µ—Ç–æ–¥—ã –¥–ª—è {url}")

        # # –ú–µ—Ç–æ–¥ 1: cloudscraper (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
        # if HAS_CLOUDSCRAPER:
        #     try:
        #         scraper = cloudscraper.create_scraper(
        #             browser={
        #                 'browser': 'chrome',
        #                 'platform': 'windows',
        #                 'desktop': True
        #             },
        #             delay=3
        #         )

        #         response = scraper.get(url, timeout=15)
        #         if response.status_code == 200 and response.text:
        #             html = response.text
        #             self.logger.info(f"‚úÖ Cloudscraper —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è {url}")
        #             self.stats['fallback_used'] += 1
        #             return self._process_html(html, url, method="cloudscraper")

        #     except Exception as e:
        #         self.logger.warning(f"‚ö†Ô∏è Cloudscraper –æ—à–∏–±–∫–∞: {e}")

        # –ú–µ—Ç–æ–¥ 2: requests —Å —Ä–∞–∑–Ω—ã–º–∏ user-agents
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]

        for user_agent in user_agents:
            try:
                headers = {
                    'User-Agent': user_agent,
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Cache-Control': 'max-age=0'
                }

                session = requests.Session()
                response = session.get(url, headers=headers, timeout=15, verify=False, allow_redirects=True)

                if response.status_code == 200 and response.text:
                    html = response.text
                    self.logger.info(f"‚úÖ Requests —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è {url}")
                    self.stats['fallback_used'] += 1
                    return self._process_html(html, url, method="requests")

            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Requests –æ—à–∏–±–∫–∞ —Å user-agent {user_agent[:30]}...: {e}")
                continue

        self.logger.error(f"‚ùå –í—Å–µ fallback –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–ª—è {url}")
        return None

    def _process_html(self, html: str, url: str, method: str = "crawl4ai") -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç HTML –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è.
        """
        if not html or len(html.strip()) < 100:
            self.logger.warning(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –ø—É—Å—Ç–æ–π HTML –¥–ª—è {url}")
            return None

        structured_html = self._extract_main_content_with_tags(html)
        metadata = self._extract_metadata(html, url)
        page_structure = self.extract_page_structure(html)
        seo_metrics = self.analyze_seo_metrics(html, url)

        result = {
            "url": url,
            "html_content": structured_html,
            "title": metadata.get("title", ""),
            "description": metadata.get("description", ""),
            "keywords": metadata.get("keywords", ""),
            "links": metadata.get("links", []),
            "content_length": len(structured_html),
            "structure": page_structure,
            "seo_metrics": seo_metrics,
            "success": True,
            "method": method,
            "timestamp": int(time.time())
        }

        return result

    async def get_structured_html(self, url: str, max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ fallback —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏.
        """
        self.stats['total_requests'] += 1
        self.logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ HTML –¥–ª—è URL: {url}")

        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º URL
        fixed_url = self.validate_and_fix_url(url)
        if not fixed_url:
            self.logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å URL: {url}")
            self.stats['failed'] += 1
            return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–æ–º–µ–Ω–∞
        is_available, error_msg = self._check_domain_availability(fixed_url)
        if not is_available:
            self.logger.warning(f"‚ö†Ô∏è –î–æ–º–µ–Ω –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {error_msg}, –ø—Ä–æ–±—É–µ–º fallback")
            result = await self._fallback_scraping(fixed_url)
            if result:
                self.stats['successful'] += 1
            else:
                self.stats['failed'] += 1
            return result

        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–æ–ø—ã—Ç–æ–∫ —Å Crawl4AI
        for attempt in range(1, max_retries + 1):
            try:
                self.logger.info(f"üì° –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries} –¥–ª—è {fixed_url}")

                async with AsyncWebCrawler(config=self.browser_config) as crawler:
                    crawler_config = CrawlerRunConfig(
                        cache_mode=CacheMode.BYPASS,
                        magic=False,
                        delay_before_return_html=2.0,
                    )

                    result = await crawler.arun(fixed_url, config=crawler_config)

                    if result and result.success and result.html:
                        self.logger.info(f"‚úÖ Crawl4AI —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è {fixed_url}")
                        processed = self._process_html(result.html, fixed_url, method="crawl4ai")
                        if processed:
                            self.stats['successful'] += 1
                            return processed
                    else:
                        error_msg = result.error_message if result else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞"
                        self.logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt} –Ω–µ —É–¥–∞–ª–∞—Å—å: {error_msg}")

            except Exception as e:
                error_str = str(e)
                if any(err in error_str for err in ["ERR_NAME_NOT_RESOLVED", "ERR_CONNECTION_REFUSED", "ERR_TIMED_OUT"]):
                    self.logger.warning(f"‚ö†Ô∏è –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}: {error_str[:100]}")
                    if attempt == max_retries:
                        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
                        result = await self._fallback_scraping(fixed_url)
                        if result:
                            self.stats['successful'] += 1
                        else:
                            self.stats['failed'] += 1
                        return result
                else:
                    self.logger.error(f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

                # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
                if attempt < max_retries:
                    await asyncio.sleep(2 ** attempt)

        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ Crawl4AI –Ω–µ —É–¥–∞–ª–∏—Å—å, –ø—Ä–æ–±—É–µ–º fallback
        self.logger.warning(f"‚ö†Ô∏è –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ Crawl4AI –Ω–µ —É–¥–∞–ª–∏—Å—å –¥–ª—è {fixed_url}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        result = await self._fallback_scraping(fixed_url)
        if result:
            self.stats['successful'] += 1
        else:
            self.stats['failed'] += 1
        return result

    def get_structured_html_sync(self, url: str, max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è get_structured_html.
        """
        try:
            return run_coro_as_sync(self.get_structured_html(url, max_retries))
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±–µ—Ä—Ç–∫–µ: {e}")
            return None

    def extract_page_structure(self, html_content: str) -> Dict[str, Any]:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        """
        structure = {
            'headers': [],
            'meta_description': None,
            'title': None,
            'images_count': 0,
            'links_count': 0,
            'word_count': 0,
            'sections': [],
            'tables_count': 0,
            'lists_count': 0,
            'forms_count': 0,
            'videos_count': 0,
            'paragraphs_count': 0
        }

        if not html_content:
            return structure

        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π
            header_hierarchy = []
            for level in range(1, 7):
                tag = f'h{level}'
                headers = soup.find_all(tag)
                for idx, h in enumerate(headers):
                    text = h.get_text(strip=True)
                    if text and len(text) > 1:
                        header_info = {
                            'level': level,
                            'text': text[:200],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                            'position': idx,
                            'id': h.get('id', ''),
                            'class': ' '.join(h.get('class', []))
                        }
                        structure['headers'].append(header_info)
                        header_hierarchy.append(header_info)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–æ –∏—Ö –ø–æ—è–≤–ª–µ–Ω–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            structure['headers'] = sorted(structure['headers'], key=lambda x: (x['level'], x['position']))

            # –ú–µ—Ç–∞-–¥–∞–Ω–Ω—ã–µ
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                structure['meta_description'] = meta_desc.get('content', '')

            # Title
            title = soup.find('title')
            if title:
                structure['title'] = title.get_text(strip=True)

            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            structure['images_count'] = len(soup.find_all('img'))
            structure['links_count'] = len(soup.find_all('a', href=True))
            structure['tables_count'] = len(soup.find_all('table'))
            structure['lists_count'] = len(soup.find_all(['ul', 'ol']))
            structure['forms_count'] = len(soup.find_all('form'))
            structure['videos_count'] = len(soup.find_all(['video', 'iframe']))
            structure['paragraphs_count'] = len(soup.find_all('p'))

            # –ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤
            text = soup.get_text()
            words = text.split()
            structure['word_count'] = len(words)

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π/—Ä–∞–∑–¥–µ–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            sections = []
            current_section = None
            for header in structure['headers']:
                if header['level'] <= 2:  # h1 –∏ h2 —Å—á–∏—Ç–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏
                    if current_section:
                        sections.append(current_section)
                    current_section = {
                        'level': header['level'],
                        'title': header['text'],
                        'subsections': []
                    }
                elif current_section and header['level'] <= 3:  # h3 –∫–∞–∫ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
                    current_section['subsections'].append(header['text'])

            if current_section:
                sections.append(current_section)

            structure['sections'] = sections

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}")

        return structure

    def analyze_seo_metrics(self, html: str, url: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ SEO –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        """
        metrics = {
            'has_h1': False,
            'h1_count': 0,
            'meta_robots': None,
            'canonical_url': None,
            'og_tags': {},
            'twitter_cards': {},
            'schema_markup': False,
            'internal_links': 0,
            'external_links': 0,
            'alt_texts_missing': 0,
            'page_speed_hints': {}
        }

        try:
            soup = BeautifulSoup(html, 'html.parser')
            parsed_url = urlparse(url)
            domain = parsed_url.netloc

            # H1 –∞–Ω–∞–ª–∏–∑
            h1_tags = soup.find_all('h1')
            metrics['h1_count'] = len(h1_tags)
            metrics['has_h1'] = metrics['h1_count'] > 0

            # Robots meta
            robots = soup.find('meta', attrs={'name': 'robots'})
            if robots:
                metrics['meta_robots'] = robots.get('content', '')

            # Canonical URL
            canonical = soup.find('link', attrs={'rel': 'canonical'})
            if canonical:
                metrics['canonical_url'] = canonical.get('href', '')

            # Open Graph —Ç–µ–≥–∏
            for og in soup.find_all('meta', property=lambda x: x and x.startswith('og:')):
                prop = og.get('property', '').replace('og:', '')
                metrics['og_tags'][prop] = og.get('content', '')

            # Twitter Cards
            for tw in soup.find_all('meta', attrs={'name': lambda x: x and x.startswith('twitter:')}):
                name = tw.get('name', '').replace('twitter:', '')
                metrics['twitter_cards'][name] = tw.get('content', '')

            # Schema.org —Ä–∞–∑–º–µ—Ç–∫–∞
            scripts = soup.find_all('script', type='application/ld+json')
            metrics['schema_markup'] = len(scripts) > 0

            # –ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫
            for link in soup.find_all('a', href=True):
                href = link['href']
                if href.startswith(('http://', 'https://')):
                    if domain in href:
                        metrics['internal_links'] += 1
                    else:
                        metrics['external_links'] += 1
                elif href.startswith('/'):
                    metrics['internal_links'] += 1

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ alt —Ç–µ–∫—Å—Ç–æ–≤ —É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            images = soup.find_all('img')
            for img in images:
                if not img.get('alt'):
                    metrics['alt_texts_missing'] += 1

            # Page speed –ø–æ–¥—Å–∫–∞–∑–∫–∏
            metrics['page_speed_hints'] = {
                'total_images': len(images),
                'images_without_lazy_loading': len([img for img in images if not img.get('loading') == 'lazy']),
                'inline_styles_count': len(soup.find_all(style=True)),
                'inline_scripts_count': len(soup.find_all('script', src=False))
            }

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ SEO –º–µ—Ç—Ä–∏–∫: {e}")

        return metrics

    def _extract_main_content_with_tags(self, html: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ HTML, —Å–æ—Ö—Ä–∞–Ω—è—è —Ç–µ–≥–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É.
        """
        if not html:
            return ""

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for element in soup(['script', 'style', 'noscript', 'meta', 'link', 'comment']):
                element.decompose()

            # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            unwanted_selectors = [
                'nav', 'footer', 'aside', 'header',
                '.navbar', '.navigation', '.nav',
                '.footer', '.header',
                '.sidebar', '.widget', '.menu',
                '.advertisement', '.ads', '.ad-container',
                '.popup', '.modal', '.overlay',
                '.cookie', '.banner',
                '#comments', '.comments'
            ]

            for selector in unwanted_selectors:
                for element in soup.select(selector):
                    element.decompose()

            # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–π –±–ª–æ–∫
            main_content = None
            content_selectors = [
                'main',
                'article',
                '[role="main"]',
                '#content',
                '.content',
                '.post',
                '.entry',
                '#main-content',
                '.main-content',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.page-content',
                '.text-content',
                '[itemprop="articleBody"]'
            ]

            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    # –í—ã–±–∏—Ä–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—Å—Ç–∞
                    main_element = max(elements, key=lambda x: len(x.get_text(strip=True)))
                    if len(main_element.get_text(strip=True)) > 100:
                        main_content = main_element
                        break

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º body
            if not main_content:
                if soup.body:
                    main_content = soup.body
                else:
                    return str(soup)

            # –û—á–∏—â–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞
            for tag in main_content.find_all(True):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                allowed_attrs = ['href', 'src', 'alt', 'title', 'id', 'class']
                tag.attrs = {key: value for key, value in tag.attrs.items() if key in allowed_attrs}

            return str(main_content)

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return html

    def _extract_metadata(self, html: str, url: str) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ HTML.
        """
        metadata = {
            'title': '',
            'description': '',
            'keywords': '',
            'author': '',
            'published_date': '',
            'modified_date': '',
            'language': '',
            'links': [],
            'emails': [],
            'phones': []
        }

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # Title
            title = soup.title
            metadata['title'] = title.get_text(strip=True) if title else ""

            # Description
            desc = soup.find('meta', attrs={'name': 'description'})
            metadata['description'] = desc.get('content', '') if desc else ""

            # Keywords
            kw = soup.find('meta', attrs={'name': 'keywords'})
            metadata['keywords'] = kw.get('content', '') if kw else ""

            # Author
            author = soup.find('meta', attrs={'name': 'author'})
            metadata['author'] = author.get('content', '') if author else ""

            # Dates
            published = soup.find('meta', property='article:published_time')
            metadata['published_date'] = published.get('content', '') if published else ""

            modified = soup.find('meta', property='article:modified_time')
            metadata['modified_date'] = modified.get('content', '') if modified else ""

            # Language
            lang_tag = soup.find('html')
            if lang_tag:
                metadata['language'] = lang_tag.get('lang', '')

            # Links (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ)
            links = set()
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                try:
                    absolute_url = urljoin(url, href)
                    if absolute_url.startswith(('http://', 'https://')):
                        links.add(absolute_url)
                except:
                    continue
            metadata['links'] = list(links)[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ

            # Emails (–±–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫)
            import re
            email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
            text = soup.get_text()
            emails = re.findall(email_pattern, text)
            metadata['emails'] = list(set(emails))[:10]

            # –¢–µ–ª–µ—Ñ–æ–Ω—ã (–±–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫)
            phone_pattern = r'[\+]?[(]?[0-9]{1,4}[)]?[-\s\.]?[(]?[0-9]{1,4}[)]?[-\s\.]?[0-9]{1,5}[-\s\.]?[0-9]{1,5}'
            phones = re.findall(phone_pattern, text)
            metadata['phones'] = list(set(phones))[:10]

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")

        return metadata

    def scrape_page(self, url: str, max_retries: int = 3) -> Optional[str]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ HTML.
        """
        result = self.get_structured_html_sync(url, max_retries)
        if result and result.get('success'):
            return result.get('html_content')
        return None

    def scrape_page_with_structure(self, url: str, max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """
        –ú–µ—Ç–æ–¥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ HTML –≤–º–µ—Å—Ç–µ —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        """
        return self.get_structured_html_sync(url, max_retries)

    def get_stats(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã —Å–∫—Ä–∞–ø–µ—Ä–∞.
        """
        stats = self.stats.copy()
        stats['success_rate'] = (
            stats['successful'] / stats['total_requests'] * 100
            if stats['total_requests'] > 0 else 0
        )
        return stats

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏
        stats = self.get_stats()
        self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫—Ä–∞–ø–µ—Ä–∞: {json.dumps(stats, indent=2)}")
        self.logger.info("StructuredHTMLScraper closed.")
